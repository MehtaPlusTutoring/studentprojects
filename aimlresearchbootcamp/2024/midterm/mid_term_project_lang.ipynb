{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Generating Answers Using LLM\n",
        "In this project we were given a data set of 27 artifacts, and ask to answer the question \"why?\"\n",
        "\n",
        "Now, \"why\" is a very broad question, so we narrowed it down to instead asking \"Why is the artifact significant to the country that gifted it?\"\n",
        "\n",
        "How did we come up with this question? Well, usually, when gift-giving, the thing that matters most is the thought that's put behind the gift. Someone who knows you very well might give you something useful, customized, or personal, while someone who doesn't know you might give a gift card.\n",
        "\n",
        "So, we came up with finding the significance of the artifact given, which would help the UN and fellow enthusiants to understand the value behind each of the gifts."
      ],
      "metadata": {
        "id": "5au8ge0zOSBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "!pip install serpapi\n",
        "!pip install requests\n",
        "!pip install -q -U google-generativeai\n",
        "import serpapi\n",
        "import requests\n",
        "import pathlib\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "from google.colab import userdata\n",
        "SERP_API_KEY=userdata.get('SERP_API_KEY')\n",
        "\n",
        "from urllib.request import Request, urlopen\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531bef07-bf95-4e9e-c3b3-fe957c5c1b30",
        "id": "NW-Jr-nAZUJy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting serpapi\n",
            "  Downloading serpapi-0.1.5-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from serpapi) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->serpapi) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->serpapi) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->serpapi) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->serpapi) (2024.7.4)\n",
            "Installing collected packages: serpapi\n",
            "Successfully installed serpapi-0.1.5\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dcAXzi3OdDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4471bcb-ef3a-4b11-e74d-ef29cb235e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "'/content/drive/MyDrive/List_gifts_for UN - List_gifts_for UN.csv'\n"
          ]
        }
      ],
      "source": [
        "#mounting my drive to access the list gifts file so we can all access the file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#data = \"/content/drive/MyDrive/List_gifts_for UN - List_gifts_for UN.csv\"\n",
        "!ls \"/content/drive/MyDrive/List_gifts_for UN - List_gifts_for UN.csv\"\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/List_gifts_for UN - List_gifts_for UN.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Our Plan\n",
        "\n",
        "We wanted to use sources and information outside of the given dataset, so we set up a function to get that.\n",
        "\n",
        "Then, we set up another function to get the data from the UN websites.\n",
        "\n",
        "To enter our data into the AI and generate a response, we made a function that we hard-coded our question into, but would also allow us to enter in the sources from the previous two steps.  \n",
        "\n",
        "But how successful was our code? FInally, we set up our last function, which was programming an accuracy metric to evaluate the AI's responses.\n"
      ],
      "metadata": {
        "id": "Wuh5Nzl_QmXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Step 1: Google Search, then scrape the text off the websites\n",
        "\n",
        "We built a function to scrape text off of the top 3 searches for each artifact item that we enter."
      ],
      "metadata": {
        "id": "zMVEJOOzTPUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#building a function to scrape the text\n",
        "def google_search_scrape(search_string): #search string is the key words that we r searching up\n",
        "\n",
        "  #set up search parameters with API key\n",
        "  params = {\n",
        "    \"q\": search_string,\n",
        "    \"hl\": \"en\",\n",
        "    \"gl\": \"us\",\n",
        "    \"num\": \"3\",\n",
        "    \"google_domain\": \"google.com\",\n",
        "    \"api_key\": SERP_API_KEY #might need to edit the restrictions on searching when it comes to the goddess of love one\n",
        "  }\n",
        "\n",
        "  search = serpapi.search(params)\n",
        "\n",
        "  #sort through relevant links (for loops)\n",
        "  relevant_links = []\n",
        "  relevant_paras = []\n",
        "  for i in search[\"organic_results\"]:\n",
        "    relevant_links.append(i[\"link\"])\n",
        "\n",
        "    #scrape text off of relevant links(for loops)\n",
        "    req = Request(\n",
        "      url=i[\"link\"],\n",
        "      headers={'User-Agent': 'Mozilla/5.0'}\n",
        "    )\n",
        "    webpage = urlopen(req).read() #copied from Miss Haripriya's collab\n",
        "    html = BeautifulSoup(webpage, 'html.parser')\n",
        "    paragraphs = html.select(\"p\")\n",
        "    paras = \"\" #setting up an empty string instead of a list, because then the paragrapghs appear in the dictionary as a list, and can't put that into dataframe\n",
        "    for para in paragraphs:\n",
        "      paras = paras + para.text\n",
        "    relevant_paras.append(paras)\n",
        "\n",
        "  return {\"links\": relevant_links, \"paras\": relevant_paras} #creating a dictionary with the links and the paragrapghs of scraped text\n",
        "\n",
        "# trial = google_search_scrape(search_string = \"amphora\")\n",
        "# print(trial)\n",
        "\n",
        "#relevant link are the top three links that the search identified\n",
        "#relevant paras are the paragrapghs of scrape text from those top three links\n",
        "#this function codes for one given link, when coding the steps needing an iteration (for loop), we need to create two new cloumns with this info, then input it into google gemini\n"
      ],
      "metadata": {
        "id": "UZbGr6gAWmYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Step 2: Scrape the text of the given website links (make a function)\n",
        "\n",
        "In this step, we want to scrape the text from the UN websites we were given. So, we made a function that returns the paragraph from the website.\n",
        "\n"
      ],
      "metadata": {
        "id": "yltG0wvETY5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    response = requests.get(url,headers=headers)\n",
        "    html = response.text\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    paragraphs = soup.find_all('p')\n",
        "    main_paragraphs = ' '.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "    return main_paragraphs\n",
        "\n",
        "#scrape(\"https://www.un.org/ungifts/content/replica-of-palenque-head\")"
      ],
      "metadata": {
        "id": "qRnFfoobTnSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Step 3: Code Gemini API function\n",
        "\n",
        "Since we now have the relevant sources, we have to program access into Gemini API, which we are using to generate responses to the problem statement.\n",
        "\n",
        "We also must set parameters that allow us to choose what sources we enter, which will be important later.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dL_z0K39UYU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "VszjJv1zD8cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY') #put your own in secrets like you did for SERP_API_KEY except this time its GOOGLE_API_KEY\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "ElVL9pqgEEIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genai_response(un_paras):\n",
        "  '''\n",
        "  Two parameters\n",
        "  un_paras - stores scraped text\n",
        "  relevant2_paras - stores paragraphs from top three most relevant websites\n",
        "  Uses query and sources to generate response\n",
        "  '''\n",
        "  model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "  response = model.generate_content(f\"\"\"\n",
        "  QUERY: Why is the artifact significant to the country that gave it?\n",
        "  SOURCES:\n",
        "  \\n{un_paras}\n",
        "   \"\"\")\n",
        "  to_markdown(response.text)\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "EVTjWGmBEEzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Accuracy Metric\n",
        "In this step, we created an accuracy metric that outputs the percentage.\n",
        "\n"
      ],
      "metadata": {
        "id": "BUGxt8J6UxOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_response_accuracy(response):\n",
        "    if 'accuracy' in response:\n",
        "        accuracy = response['accuracy']\n",
        "        if isinstance(accuracy, (int, float)):\n",
        "            # Assuming the accuracy is represented as a number (percentage, score, etc.)\n",
        "            # You can add more specific checks based on the actual response structure\n",
        "            if accuracy >= 0 and accuracy <= 100:  # Example condition for percentage accuracy\n",
        "                return True\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "sIaQCjuqUz6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Step 5: Code an iteration that simultaenously does the following\n",
        "1) Creates new columns for the paragrapghs of scraped text\n",
        "\n",
        "2) Inputs the data for each into the gemini API function\n",
        "\n",
        "3) Takes the response and stores it in the column\n",
        "\n",
        "4) Takes each response and runs it through the accuracy metric\n",
        "\n",
        "5) Prints out the accuracy and creats a new column with it\n",
        "\n",
        "6) Creates a new csv with all this information for easy access\n"
      ],
      "metadata": {
        "id": "XVuTZr0M89Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#THE ISSUE: Okay, so the main problem is the genai function because if I input a lot of rows it gives me a forbidden error, so now I need to figure out how to enter the dataset\n",
        "#into this loop without getting a forbidden error\n",
        "#POSSIBLE SOLUTION: Breakup the csv into rows of 4 (tedious)\n",
        "\n",
        "temp = data.head(5) #note: when running the acutal thing, fix this because it only lets us run one row of the dataset, but only after we try it to make sure it works\n",
        "\n",
        "#declaring lists to put the information in; this way, i can put it in the columns without problems\n",
        "links = []\n",
        "paras = []\n",
        "un_paras = []\n",
        "llm_resp = []\n",
        "accuracy_score = []\n",
        "\n",
        "#coding a for loop that does the aove steps\n",
        "for index, row in temp.iterrows():\n",
        "  result = google_search_scrape(search_string = row[\"Name\"]) #only one we don't need to fix since serp_api thing is already coded\n",
        "  links.append(result[\"links\"]) #\n",
        "  paras.append(\"\\n\".join(result[\"paras\"]))\n",
        "  un_res = scrape(url = row[\"Link to Museum\"]) #FIXED\n",
        "  un_paras.append(un_res)\n",
        "  t_llm_resp = genai_response(un_paras = un_res, relevant2_paras = paras) #how we are storing the values in a variable to use in accuracy score append\n",
        "  llm_resp.append(t_llm_resp) #how we r making the column\n",
        "\n",
        "  accuracy_score.append(check_response_accuracy(response = t_llm_resp))\n",
        "\n",
        "\n",
        "#making the columns\n",
        "temp[\"links\"] = links\n",
        "temp[\"paras\"] = paras\n",
        "temp[\"un_paras\"] = un_paras\n",
        "temp[\"llm_resp\"] = llm_resp\n",
        "temp[\"accuracy_score\"] = accuracy_score\n",
        "\n",
        "#output\n",
        "#\n",
        "\n",
        "#data2 = pd.read_csv(temp.to_csv())\n",
        "#where accuracy is an integer/ percent\n",
        "temp.to_csv(\"/content/drive/MyDrive/List_gifts_for UN Output.csv\")"
      ],
      "metadata": {
        "id": "dudBkIRp96Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = data #note: when running the acutal thing, fix this because it only lets us run one row of the dataset, but only after we try it to make sure it works\n",
        "\n",
        "#declaring lists to put the information in; this way, i can put it in the columns without problems\n",
        "\n",
        "un_paras = []\n",
        "llm_resp = []\n",
        "accuracy_score = []\n",
        "\n",
        "#coding a for loop that does the aove steps\n",
        "for index, row in temp.iterrows():\n",
        "  #result = google_search_scrape(search_string = row[\"Name\"]) #only one we don't need to fix since serp_api thing is already coded\n",
        "  #links.append(result[\"links\"]) #\n",
        "  #paras.append(\"\\n\".join(result[\"paras\"]))\n",
        "  un_res = scrape(url = row[\"Link to Museum\"]) #FIXED\n",
        "  un_paras.append(un_res)\n",
        "  t_llm_resp = genai_response(un_paras = un_res) #how we are storing the values in a variable to use in accuracy score append\n",
        "  llm_resp.append(t_llm_resp) #how we r making the column\n",
        "\n",
        "  accuracy_score.append(check_response_accuracy(response = t_llm_resp))\n",
        "\n",
        "\n",
        "#making the columns\n",
        "# temp[\"links\"] = links\n",
        "# temp[\"paras\"] = paras\n",
        "temp[\"un_paras\"] = un_paras\n",
        "temp[\"llm_resp\"] = llm_resp\n",
        "temp[\"accuracy_score\"] = accuracy_score\n",
        "\n",
        "#output\n",
        "#\n",
        "\n",
        "#data2 = pd.read_csv(temp.to_csv())\n",
        "#where accuracy is an integer/ percent\n",
        "temp.to_csv(\"/content/drive/MyDrive/List_gifts_for UN Output.csv\")"
      ],
      "metadata": {
        "id": "fN5YO_C6KUVh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}